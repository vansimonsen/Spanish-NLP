{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',1000)\n",
    "from lxml import objectify\n",
    "import numpy as np\n",
    "from pandas import read_pickle, read_csv, DataFrame\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer      \n",
    "\n",
    "\n",
    "#Required packages from nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV file or create from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('datasets/csv/general-tweets-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-train-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], \n",
    "                       [tweet.content.text, tweet.sentiments.polarity.value.text, \n",
    "                        tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_train = general_tweets_corpus_train.append(row_s)\n",
    "    general_tweets_corpus_train.to_csv('datasets/csv/general-tweets-train-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>Si. Señor.. Ahi vamos RT @jerry_bob31: @AlejandroSanz Vamos por el Nuevo Disco!!</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Es un muñeco! (pero la verdad es que lo han clavao...) http://t.co/SFtib43B</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>#quemiedo como nos \"rescaten\"</td>\n",
       "      <td>NEU</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Tranquilidad en las carreteras en el segundo día de Puente de la Constitución - http://t.co/NRk2uftQ</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>El primero. @elmundoes: La juez envía a prisión al exdirector de Trabajo andaluz imputado por el caso de los ERE falsos http://t.co/OadFKKj2</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           content  \\\n",
       "2664                                                              Si. Señor.. Ahi vamos RT @jerry_bob31: @AlejandroSanz Vamos por el Nuevo Disco!!   \n",
       "447                                                                    Es un muñeco! (pero la verdad es que lo han clavao...) http://t.co/SFtib43B   \n",
       "4134                                                                                                                 #quemiedo como nos \"rescaten\"   \n",
       "82                                            Tranquilidad en las carreteras en el segundo día de Puente de la Constitución - http://t.co/NRk2uftQ   \n",
       "5538  El primero. @elmundoes: La juez envía a prisión al exdirector de Trabajo andaluz imputado por el caso de los ERE falsos http://t.co/OadFKKj2   \n",
       "\n",
       "     polarity  agreement  \n",
       "2664        P  AGREEMENT  \n",
       "447         P  AGREEMENT  \n",
       "4134      NEU  AGREEMENT  \n",
       "82         P+  AGREEMENT  \n",
       "5538       N+  AGREEMENT  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus = pd.concat([general_tweets_corpus_train])\n",
    "tweets_corpus = tweets_corpus.query('agreement != \"DISAGREEMENT\" and polarity != \"NONE\"')\n",
    "tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]\n",
    "tweets_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_test = pd.read_csv('datasets/csv/general-tweets-test1k.csv')#, encoding='utf-8')\n",
    "except:\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-test1k.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content'], [tweet.content.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_test = general_tweets_corpus_test.append(row_s)\n",
    "    general_tweets_corpus_test.to_csv('datasets/csv/general-tweets-test1k.csv', index=False)#, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>\"@luiscalvo92: has leído 'Desde el otro lado del escaparate' de Toni Segarra?\" Claro. Lo mejor sobre publicidad escrito hasta la fecha.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Wow! Cómo se la ha jugado mi hermandad, la Vera Cruz, saliendo hoy. Lo ha dado todo y solo ha podido recorrer una calle.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>Tampoco respeto ni acato una ley electoral que supone negar el principio democrático de una persona, un voto. Fraudulenta.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>Mil gracias ;-))) RT @cibeles5: @mariviromero #FF Feliz congreso</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Los 10.000 millones de recortes en sanidad y educación sin competencias para ellos, solo demuestra acojono gubernamental. Así, leche segura.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          content  \\\n",
       "454       \"@luiscalvo92: has leído 'Desde el otro lado del escaparate' de Toni Segarra?\" Claro. Lo mejor sobre publicidad escrito hasta la fecha.   \n",
       "971                      Wow! Cómo se la ha jugado mi hermandad, la Vera Cruz, saliendo hoy. Lo ha dado todo y solo ha podido recorrer una calle.   \n",
       "680                    Tampoco respeto ni acato una ley electoral que supone negar el principio democrático de una persona, un voto. Fraudulenta.   \n",
       "613                                                                              Mil gracias ;-))) RT @cibeles5: @mariviromero #FF Feliz congreso   \n",
       "993  Los 10.000 millones de recortes en sanidad y educación sin competencias para ellos, solo demuestra acojono gubernamental. Así, leche segura.   \n",
       "\n",
       "     polarity  \n",
       "454       NaN  \n",
       "971       NaN  \n",
       "680       NaN  \n",
       "613       NaN  \n",
       "993       NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test = pd.concat([general_tweets_corpus_test])\n",
    "\n",
    "tweets_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tagged_tweets_corpus_test = pd.read_csv('datasets/csv/general-tweets-test1k-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-test1k-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    tagged_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        tagged_tweets_corpus_test = tagged_tweets_corpus_test.append(row_s)\n",
    "    tagged_tweets_corpus_test.to_csv('datasets/csv/general-tweets-test1k-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>En @poloniatv3 estrenando nuevo personaje. El ministro DE GUINDOS!! Veremos q tal recortamos...;))</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>Aparato de hierro (Rubalcaba forma un bloque compacto para  la peor situación del PSOE) Blog El Patio del Congreso http://t.co/64HaGHan</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>\"Os pido el apoyo para un proyecto nuevo en una sociedad que cambia\".</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>El blog de @mariarojo82  dietas sanas, nutrición y falsos mitos | Dietas sin milagros - Blog diariosur.es http://t.co/LrAmWEpI</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>PArece útil! Gracias.“@susalonso: Una sola página para descubrir las noticias más leídas de principales medios online http://t.co/9SYIL5b1”</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         content  \\\n",
       "289                                           En @poloniatv3 estrenando nuevo personaje. El ministro DE GUINDOS!! Veremos q tal recortamos...;))   \n",
       "520      Aparato de hierro (Rubalcaba forma un bloque compacto para  la peor situación del PSOE) Blog El Patio del Congreso http://t.co/64HaGHan   \n",
       "299                                                                        \"Os pido el apoyo para un proyecto nuevo en una sociedad que cambia\".   \n",
       "415               El blog de @mariarojo82  dietas sanas, nutrición y falsos mitos | Dietas sin milagros - Blog diariosur.es http://t.co/LrAmWEpI   \n",
       "908  PArece útil! Gracias.“@susalonso: Una sola página para descubrir las noticias más leídas de principales medios online http://t.co/9SYIL5b1”   \n",
       "\n",
       "    polarity  \n",
       "289       P+  \n",
       "520        N  \n",
       "299       P+  \n",
       "415       P+  \n",
       "908       P+  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tagged = pd.concat([tagged_tweets_corpus_test])\n",
    "tweets_tagged = tweets_tagged.query('polarity != \"NONE\"')\n",
    "diff = np.setdiff1d(tweets_test.index.values, tweets_tagged.index.values)\n",
    "\n",
    "tweets_test = tweets_test.drop(diff)\n",
    "tweets_tagged.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Stems Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stem: Cut word in root (wait: wait, waited: wait, waiting: wait)\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Each word is a token\n",
    "def tokenize(text):\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stopwords: Empty word (i.e articles)\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "#Non Words: Symbols and Numbers\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model (Linear SVM) and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.53994\n",
      "0    0.46006\n",
      "Name: polarity_bin, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/pandas/core/indexing.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>agreement</th>\n",
       "      <th>polarity_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>RT “@sgueina: He apoyado la petición para q a “@sanchez_sonia Rajoy le condecore con la Gran Cruz Carlos III  y Collar Isabel La Católica”</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>Muy bueno Reixa: la fabricación de placas inaugurales en la época de don Manuel, una industria boyante #cuandoeramoscultos</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>El alcalde de Olula, del PP, agradece que el nombre del pueblo salga en los medios con la presentación de Chacón. http://t.co/lLIQG8gt</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         content  \\\n",
       "1551  RT “@sgueina: He apoyado la petición para q a “@sanchez_sonia Rajoy le condecore con la Gran Cruz Carlos III  y Collar Isabel La Católica”   \n",
       "5631                  Muy bueno Reixa: la fabricación de placas inaugurales en la época de don Manuel, una industria boyante #cuandoeramoscultos   \n",
       "1892      El alcalde de Olula, del PP, agradece que el nombre del pueblo salga en los medios con la presentación de Chacón. http://t.co/lLIQG8gt   \n",
       "\n",
       "     polarity  agreement  polarity_bin  \n",
       "1551       P+  AGREEMENT             1  \n",
       "5631       P+  AGREEMENT             1  \n",
       "1892       P+  AGREEMENT             1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binarizing\n",
    "\n",
    "tweets_corpus['polarity_bin'] = 0\n",
    "index = tweets_corpus.polarity.isin(['P', 'P+'])\n",
    "tweets_corpus.polarity_bin.loc[index] = 1\n",
    "print tweets_corpus.polarity_bin.value_counts(normalize=True)\n",
    "\n",
    "tweets_test['polarity_bin'] = 0\n",
    "\n",
    "tweets_tagged['polarity_bin'] = 0\n",
    "index = tweets_tagged.polarity.isin(['P', 'P+'])\n",
    "tweets_tagged.polarity_bin.loc[index] = 1\n",
    "tweets_tagged.polarity_bin.value_counts(normalize=True)\n",
    "\n",
    "y = tweets_tagged.polarity_bin.values\n",
    "\n",
    "tweets_corpus.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search.fit(tweets_corpus.content, tweets_corpus.polarity_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.5, max_features=1000, min_df=10,\n",
       "        ngram_range=(1, 2), preprocessor=None,\n",
       "        stop_words=[u'de', ...e', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = {'vect__ngram_range': (1, 2), 'cls__loss': 'hinge', 'vect__max_df': 0.5\n",
    " , 'cls__max_iter': 1000, 'vect__min_df': 10, 'vect__max_features': 1000\n",
    " , 'cls__C': 0.2}\n",
    "\n",
    "best_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=1000\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='hinge',max_iter=1000,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])\n",
    "\n",
    "best_pipe.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "#best_test = tweets_test\n",
    "#best_test['polarity_bin'] = best_pipe.predict(best_test.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80398162327718226"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_b = best_test.polarity_bin.values\n",
    "best_result = np.abs(y_b - y)\n",
    "np.bincount(best_result)[0]/float(best_result.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords,\n",
    "            min_df = 50,\n",
    "            max_df = 1.9,\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=1000\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='squared_hinge',max_iter=1000,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])\n",
    "\n",
    "pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "tweets_test['polarity_bin'] = pipeline.predict(tweets_test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71975497702909652"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t = tweets_test.polarity_bin.values\n",
    "result = np.abs(y_t - y)\n",
    "np.bincount(result)[0]/float(result.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68227481108362864"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "\n",
    "scores = cross_val_score(p, tweets_corpus.content, tweets_corpus.polarity_bin, cv=5)\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73773511948822568"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores = cross_val_score(best_pipe, tweets_corpus.content, tweets_corpus.polarity_bin, cv=5)\n",
    "\n",
    "np.mean(best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    re_test = pd.read_csv('datasets/csv/MiniREa.csv', encoding='utf-8')\n",
    "except:\n",
    "    xml = objectify.parse(open('datasets/xml/MiniRE.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    re_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content'], [tweet.content.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        if row_s[0] != None:\n",
    "            row_s.name = i\n",
    "            re_test = re_test.append(row_s)\n",
    "    re_test.to_csv('datasets/csv/MiniRE.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re = read_pickle('../twitterProject/Data/ProcessedTags/pickle/re')\n",
    "rl = read_pickle('../twitterProject/Data/ProcessedTags/pickle/rl')\n",
    "rc = read_pickle('../twitterProject/Data/ProcessedTags/pickle/rc')\n",
    "rt = read_pickle('../twitterProject/Data/ProcessedTags/pickle/rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "re['polarity'] = best_pipe.predict(re.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "rc['polarity'] = best_pipe.predict(rc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "rl['polarity'] = best_pipe.predict(rl.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "rt['polarity'] = best_pipe.predict(rt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.to_pickle('../twitterProject/Data/ProcessedTags/pickle/re')\n",
    "rc.to_pickle('../twitterProject/Data/ProcessedTags/pickle/rc')\n",
    "rt.to_pickle('../twitterProject/Data/ProcessedTags/pickle/rt')\n",
    "rl.to_pickle('../twitterProject/Data/ProcessedTags/pickle/rl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
