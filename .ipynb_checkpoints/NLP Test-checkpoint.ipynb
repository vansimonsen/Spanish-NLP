{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',1000)\n",
    "from lxml import objectify\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer      \n",
    "\n",
    "\n",
    "#Required packages from nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV file or create from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('datasets/csv/general-tweets-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-train-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], \n",
    "                       [tweet.content.text, tweet.sentiments.polarity.value.text, \n",
    "                        tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_train = general_tweets_corpus_train.append(row_s)\n",
    "    general_tweets_corpus_train.to_csv('datasets/csv/general-tweets-train-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>Buenos días ;-)) q tengáis un sábado estupendo ;-))</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>RT @Majo_eltren: @Los40_Spain quiero escuchar #GeneracionPerdida del disco #Positive Generation, Voces x un ... http://t.co/MOyaNTvd</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>Mira, Rajoy, aprende de los tecnócratas. http://t.co/HDUZqqJx</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>Otro presupuesto es posible. Hemos hablado con Ghesta y Rallo. @TelediarioInter 20:30</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Estoy harto del clásico. Qué cansino todo el mundo con el partido. Creo que Pep saldrá con defensa de 3.</td>\n",
       "      <td>N</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   content  \\\n",
       "4358                                                                                   Buenos días ;-)) q tengáis un sábado estupendo ;-))   \n",
       "4194  RT @Majo_eltren: @Los40_Spain quiero escuchar #GeneracionPerdida del disco #Positive Generation, Voces x un ... http://t.co/MOyaNTvd   \n",
       "1087                                                                         Mira, Rajoy, aprende de los tecnócratas. http://t.co/HDUZqqJx   \n",
       "6936                                                 Otro presupuesto es posible. Hemos hablado con Ghesta y Rallo. @TelediarioInter 20:30   \n",
       "373                               Estoy harto del clásico. Qué cansino todo el mundo con el partido. Creo que Pep saldrá con defensa de 3.   \n",
       "\n",
       "     polarity  agreement  \n",
       "4358       P+  AGREEMENT  \n",
       "4194        P  AGREEMENT  \n",
       "1087       N+  AGREEMENT  \n",
       "6936        P  AGREEMENT  \n",
       "373         N  AGREEMENT  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus = pd.concat([general_tweets_corpus_train])\n",
    "tweets_corpus = tweets_corpus.query('agreement != \"DISAGREEMENT\" and polarity != \"NONE\"')\n",
    "tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]\n",
    "tweets_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_test = pd.read_csv('datasets/csv/general-tweets-test1k.csv')#, encoding='utf-8')\n",
    "except:\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-test1k.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content'], [tweet.content.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_test = general_tweets_corpus_test.append(row_s)\n",
    "    general_tweets_corpus_test.to_csv('datasets/csv/general-tweets-test1k.csv', index=False)#, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Lo siento muchísimo, era estupenda “@loriferrer: @pedroj_ramirez  falleció ayer #raimundadepeñafort nuestra juez favorita. Descanse en paz”</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Portada de Liberation  http://t.co/2J4MNxba</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>La diferencia entre PP y PSOE es solo de 40.000 votos, un 1% de diferencia.  IU sube 110.000.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Pensiones. Garantizar sostenibilidad. Combatir prejubilaciones encubiertas. Incentivar prolongación voluntaria de la vida laboral.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Como en el caso Dreyfus, situados entre la verdad y el prestigio de la Institución, al final ni verdad  ni prestigio.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         content  \\\n",
       "516  Lo siento muchísimo, era estupenda “@loriferrer: @pedroj_ramirez  falleció ayer #raimundadepeñafort nuestra juez favorita. Descanse en paz”   \n",
       "841                                                                                                  Portada de Liberation  http://t.co/2J4MNxba   \n",
       "891                                                La diferencia entre PP y PSOE es solo de 40.000 votos, un 1% de diferencia.  IU sube 110.000.   \n",
       "129           Pensiones. Garantizar sostenibilidad. Combatir prejubilaciones encubiertas. Incentivar prolongación voluntaria de la vida laboral.   \n",
       "573                        Como en el caso Dreyfus, situados entre la verdad y el prestigio de la Institución, al final ni verdad  ni prestigio.   \n",
       "\n",
       "     polarity  \n",
       "516       NaN  \n",
       "841       NaN  \n",
       "891       NaN  \n",
       "129       NaN  \n",
       "573       NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test = pd.concat([general_tweets_corpus_test])\n",
    "\n",
    "tweets_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tagged_tweets_corpus_test = pd.read_csv('datasets/csv/general-tweets-test1k-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-test1k-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    tagged_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        tagged_tweets_corpus_test = tagged_tweets_corpus_test.append(row_s)\n",
    "    tagged_tweets_corpus_test.to_csv('datasets/csv/general-tweets-test1k-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Somosaguas Place! Hahahahahahahahah genial! @LosClones me muero de la risa! Hahahahaha brutal</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Entre los méritos de De Guindos está gestionar la salida a bolsa de la CAM (desde Lehman Brothers) ¡2 quiebras en 1! http://t.co/Q5DPL4LC</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>Buenas chicos!! Esta noche en sala kairo en antequera con mi primer tema #Quitateeltop dale duro!!!!</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sembrar dudas sobre la autoría del 11M, para negar su derrota electoral, mientras se desprecia a las familias de las víctimas, es vergonzoso</td>\n",
       "      <td>N+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Y su responsabilidad? @PepeGrinan admite que el caso de los ERE puede tener un coste electoral «grande» http://t.co/DTeeck2l</td>\n",
       "      <td>N+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          content  \\\n",
       "307                                                 Somosaguas Place! Hahahahahahahahah genial! @LosClones me muero de la risa! Hahahahaha brutal   \n",
       "183     Entre los méritos de De Guindos está gestionar la salida a bolsa de la CAM (desde Lehman Brothers) ¡2 quiebras en 1! http://t.co/Q5DPL4LC   \n",
       "978                                          Buenas chicos!! Esta noche en sala kairo en antequera con mi primer tema #Quitateeltop dale duro!!!!   \n",
       "37   Sembrar dudas sobre la autoría del 11M, para negar su derrota electoral, mientras se desprecia a las familias de las víctimas, es vergonzoso   \n",
       "354                  Y su responsabilidad? @PepeGrinan admite que el caso de los ERE puede tener un coste electoral «grande» http://t.co/DTeeck2l   \n",
       "\n",
       "    polarity  \n",
       "307       P+  \n",
       "183        P  \n",
       "978       P+  \n",
       "37        N+  \n",
       "354       N+  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tagged = pd.concat([tagged_tweets_corpus_test])\n",
    "tweets_tagged = tweets_tagged.query('polarity != \"NONE\"')\n",
    "diff = np.setdiff1d(tweets_test.index.values, tweets_tagged.index.values)\n",
    "\n",
    "tweets_test = tweets_test.drop(diff)\n",
    "tweets_tagged.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Stems Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stem: Cut word in root (wait: wait, waited: wait, waiting: wait)\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Each word is a token\n",
    "def tokenize(text):\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stopwords: Empty word (i.e articles)\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "#Non Words: Symbols and Numbers\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model (Linear SVM) and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.53994\n",
      "0    0.46006\n",
      "Name: polarity_bin, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>agreement</th>\n",
       "      <th>polarity_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>Gran discurso de @javierarenas_pp hoy en el congreso del @partidopopular. Cada dia es mayor su capacidad de llegar a la gente y convencer</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>Un besazo hasta Sevilla. RT @PAQUIKAVITO: @Jas_Sevilla @siempreesdnoche @alejandrosanz gracias salao!!! Besos pa Sevilla !!!</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>.@diegocruz_: “El Ayuntamiento va con retraso en la simplificación de licencias”. Esta es la realidad. http://t.co/G6M0IkWp</td>\n",
       "      <td>N</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        content  \\\n",
       "4457  Gran discurso de @javierarenas_pp hoy en el congreso del @partidopopular. Cada dia es mayor su capacidad de llegar a la gente y convencer   \n",
       "5081               Un besazo hasta Sevilla. RT @PAQUIKAVITO: @Jas_Sevilla @siempreesdnoche @alejandrosanz gracias salao!!! Besos pa Sevilla !!!   \n",
       "7191                .@diegocruz_: “El Ayuntamiento va con retraso en la simplificación de licencias”. Esta es la realidad. http://t.co/G6M0IkWp   \n",
       "\n",
       "     polarity  agreement  polarity_bin  \n",
       "4457       P+  AGREEMENT             1  \n",
       "5081       P+  AGREEMENT             1  \n",
       "7191        N  AGREEMENT             0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binarizing\n",
    "\n",
    "tweets_corpus['polarity_bin'] = 0\n",
    "index = tweets_corpus.polarity.isin(['P', 'P+'])\n",
    "tweets_corpus.polarity_bin.loc[index] = 1\n",
    "print tweets_corpus.polarity_bin.value_counts(normalize=True)\n",
    "\n",
    "tweets_test['polarity_bin'] = 0\n",
    "\n",
    "tweets_tagged['polarity_bin'] = 0\n",
    "index = tweets_tagged.polarity.isin(['P', 'P+'])\n",
    "tweets_tagged.polarity_bin.loc[index] = 1\n",
    "tweets_tagged.polarity_bin.value_counts(normalize=True)\n",
    "\n",
    "y = tweets_tagged.polarity_bin.values\n",
    "\n",
    "tweets_corpus.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search.fit(tweets_corpus.content, tweets_corpus.polarity_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "best_params = {'vect__ngram_range': (1, 2), 'cls__loss': 'hinge', 'vect__max_df': 0.5\n",
    " , 'cls__max_iter': 1000, 'vect__min_df': 10, 'vect__max_features': 1000\n",
    " , 'cls__C': 0.2}\n",
    "\n",
    "best_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=1000\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='hinge',max_iter=1000,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])\n",
    "\n",
    "best_pipe.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "best_test = tweets_test\n",
    "best_test['polarity_bin'] = best_pipe.predict(best_test.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80398162327718226"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_b = best_test.polarity_bin.values\n",
    "best_result = np.abs(y_b - y)\n",
    "np.bincount(best_result)[0]/float(best_result.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords,\n",
    "            min_df = 50,\n",
    "            max_df = 1.9,\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=1000\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='squared_hinge',max_iter=1000,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])\n",
    "\n",
    "pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "tweets_test['polarity_bin'] = pipeline.predict(tweets_test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71975497702909652"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t = tweets_test.polarity_bin.values\n",
    "result = np.abs(y_t - y)\n",
    "np.bincount(result)[0]/float(result.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68227481108362864"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "\n",
    "scores = cross_val_score(p, tweets_corpus.content, tweets_corpus.polarity_bin, cv=5)\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73773511948822568"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores = cross_val_score(best_pipe, tweets_corpus.content, tweets_corpus.polarity_bin, cv=5)\n",
    "\n",
    "np.mean(best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
