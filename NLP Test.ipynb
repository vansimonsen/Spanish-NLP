{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',1000)\n",
    "from lxml import objectify\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer      \n",
    "\n",
    "\n",
    "#Required packages from nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV file or create from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_train = pd.read_csv('datasets/csv/general-tweets-train-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-train-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_train = pd.DataFrame(columns=('content', 'polarity', 'agreement'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], \n",
    "                       [tweet.content.text, tweet.sentiments.polarity.value.text, \n",
    "                        tweet.sentiments.polarity.type.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_train = general_tweets_corpus_train.append(row_s)\n",
    "    general_tweets_corpus_train.to_csv('datasets/csv/general-tweets-train-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>#cartaalosreyesmagos. Rajoy necesita una sala de prensa. La q hay no le debe de gustar</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6156</th>\n",
       "      <td>Anda! Te pones un parche magnético en la piel y vibra cuando se recibe una llamada en el móvil. http://t.co/BDhLtz3B”  !! Que disparate!!</td>\n",
       "      <td>N</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>graciasss... Trabajamos x ello “@EDU_AM08: Felicidades!! Presidente aver si con el trabajo de todos mejoramos la situación.”</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>Estoy en un sitio en el que no puedo ver el partido. Veo que el Madrid va ganando. Contadme por favor lo esencial. Es clave ganar hoy.</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>¡Noticias descombacantes! está disponible! http://t.co/pdWFYyRa ? Historias del día por @burdinjaun</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        content  \\\n",
       "1800                                                     #cartaalosreyesmagos. Rajoy necesita una sala de prensa. La q hay no le debe de gustar   \n",
       "6156  Anda! Te pones un parche magnético en la piel y vibra cuando se recibe una llamada en el móvil. http://t.co/BDhLtz3B”  !! Que disparate!!   \n",
       "2065               graciasss... Trabajamos x ello “@EDU_AM08: Felicidades!! Presidente aver si con el trabajo de todos mejoramos la situación.”   \n",
       "6777     Estoy en un sitio en el que no puedo ver el partido. Veo que el Madrid va ganando. Contadme por favor lo esencial. Es clave ganar hoy.   \n",
       "1868                                        ¡Noticias descombacantes! está disponible! http://t.co/pdWFYyRa ? Historias del día por @burdinjaun   \n",
       "\n",
       "     polarity  agreement  \n",
       "1800       N+  AGREEMENT  \n",
       "6156        N  AGREEMENT  \n",
       "2065       P+  AGREEMENT  \n",
       "6777       P+  AGREEMENT  \n",
       "1868        P  AGREEMENT  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus = pd.concat([general_tweets_corpus_train])\n",
    "tweets_corpus = tweets_corpus.query('agreement != \"DISAGREEMENT\" and polarity != \"NONE\"')\n",
    "tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]\n",
    "tweets_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    general_tweets_corpus_test = pd.read_csv('datasets/csv/general-tweets-test1k.csv')#, encoding='utf-8')\n",
    "except:\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-test1k.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    general_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content'], [tweet.content.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        general_tweets_corpus_test = general_tweets_corpus_test.append(row_s)\n",
    "    general_tweets_corpus_test.to_csv('datasets/csv/general-tweets-test1k.csv', index=False)#, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Mil gracias por vuestras felicitaciones!!! #MasViejos #MasSabios?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>Rubalcaba quiere girar más a la izquierda,pues el PP debe ir  más a la derecha o nos seguirán imponiendo su ruinoso modelo de sociedad..</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Muy bien la gente de antequera pero malisimos los responsables de la sala kairo nos han dejado tirados sin billetes en el hotel.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>@lazaroelmundo feliz año, amigo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Este premio reconoce que Valencia es una ciudad pensada para la integración de todos, y en esa línea seguimos... http://t.co/8Jiu1pjW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      content  \\\n",
       "287                                                                         Mil gracias por vuestras felicitaciones!!! #MasViejos #MasSabios?   \n",
       "511  Rubalcaba quiere girar más a la izquierda,pues el PP debe ir  más a la derecha o nos seguirán imponiendo su ruinoso modelo de sociedad..   \n",
       "982          Muy bien la gente de antequera pero malisimos los responsables de la sala kairo nos han dejado tirados sin billetes en el hotel.   \n",
       "225                                                                                                           @lazaroelmundo feliz año, amigo   \n",
       "994     Este premio reconoce que Valencia es una ciudad pensada para la integración de todos, y en esa línea seguimos... http://t.co/8Jiu1pjW   \n",
       "\n",
       "     polarity  \n",
       "287       NaN  \n",
       "511       NaN  \n",
       "982       NaN  \n",
       "225       NaN  \n",
       "994       NaN  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test = pd.concat([general_tweets_corpus_test])\n",
    "\n",
    "tweets_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tagged_tweets_corpus_test = pd.read_csv('datasets/csv/general-tweets-test1k-tagged.csv', encoding='utf-8')\n",
    "except:\n",
    "\n",
    "    from lxml import objectify\n",
    "    xml = objectify.parse(open('datasets/xml/general-tweets-test1k-tagged.xml'))\n",
    "    #sample tweet object\n",
    "    root = xml.getroot()\n",
    "    tagged_tweets_corpus_test = pd.DataFrame(columns=('content', 'polarity'))\n",
    "    tweets = root.getchildren()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        row = dict(zip(['content', 'polarity', 'agreement'], [tweet.content.text, tweet.sentiments.polarity.value.text]))\n",
    "        row_s = pd.Series(row)\n",
    "        row_s.name = i\n",
    "        tagged_tweets_corpus_test = tagged_tweets_corpus_test.append(row_s)\n",
    "    tagged_tweets_corpus_test.to_csv('datasets/csv/general-tweets-test1k-tagged.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Rajoy contesta a los portavoces en el pleno del Congreso como si le molestara estar aquí y estuviera pasando un trámite que le aburre</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>@ccifuentes @PaquiVicente buen sábado para las dos.</td>\n",
       "      <td>P+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Todos han jurado.soraya se ha equivocado,De Guindos ha puesto la mano sobre el papel y Morenes,taconazo muy militar.</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>El acuerdo de recorte de derechos laborales aprobada por PP, CIU, UPN y FAC, es un ataque brutal que debe tener justa respuesta global.</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Argentina estaria cerca de mi aunque estuviera en otro planeta. RT @CecVill: @AlejandroSanz @adasaro YO QUIERO UN ... http://t.co/J7XwvA4F</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        content  \\\n",
       "798       Rajoy contesta a los portavoces en el pleno del Congreso como si le molestara estar aquí y estuviera pasando un trámite que le aburre   \n",
       "716                                                                                         @ccifuentes @PaquiVicente buen sábado para las dos.   \n",
       "161                        Todos han jurado.soraya se ha equivocado,De Guindos ha puesto la mano sobre el papel y Morenes,taconazo muy militar.   \n",
       "751     El acuerdo de recorte de derechos laborales aprobada por PP, CIU, UPN y FAC, es un ataque brutal que debe tener justa respuesta global.   \n",
       "995  Argentina estaria cerca de mi aunque estuviera en otro planeta. RT @CecVill: @AlejandroSanz @adasaro YO QUIERO UN ... http://t.co/J7XwvA4F   \n",
       "\n",
       "    polarity  \n",
       "798        N  \n",
       "716       P+  \n",
       "161        N  \n",
       "751        N  \n",
       "995        P  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tagged = pd.concat([tagged_tweets_corpus_test])\n",
    "tweets_tagged = tweets_tagged.query('polarity != \"NONE\"')\n",
    "diff = np.setdiff1d(tweets_test.index.values, tweets_tagged.index.values)\n",
    "\n",
    "tweets_test = tweets_test.drop(diff)\n",
    "tweets_tagged.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Stems Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stem: Cut word in root (wait: wait, waited: wait, waiting: wait)\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Each word is a token\n",
    "def tokenize(text):\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stopwords: Empty word (i.e articles)\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "\n",
    "#Non Words: Symbols and Numbers\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model (Linear SVM) and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.53994\n",
      "0    0.46006\n",
      "Name: polarity_bin, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Binarizing\n",
    "\n",
    "tweets_corpus['polarity_bin'] = 0\n",
    "index = tweets_corpus.polarity.isin(['P', 'P+'])\n",
    "tweets_corpus.polarity_bin.loc[index] = 1\n",
    "print tweets_corpus.polarity_bin.value_counts(normalize=True)\n",
    "\n",
    "tweets_test['polarity_bin'] = 0\n",
    "\n",
    "tweets_tagged['polarity_bin'] = 0\n",
    "index = tweets_tagged.polarity.isin(['P', 'P+'])\n",
    "tweets_tagged.polarity_bin.loc[index] = 1\n",
    "tweets_tagged.polarity_bin.value_counts(normalize=True)\n",
    "\n",
    "y = tweets_tagged.polarity_bin.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b0ae49d525ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0;31m# a working pool as they expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search.fit(tweets_corpus.content, tweets_corpus.polarity_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords,\n",
    "            min_df = 50,\n",
    "            max_df = 1.9,\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=1000\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='squared_hinge',max_iter=1000,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])\n",
    "\n",
    "pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "tweets_test['polarity_bin'] = pipeline.predict(tweets_test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68227481108362864"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pipeline.fit(X_train.content, X_train.polarity_bin)\n",
    "\n",
    "scores = cross_val_score(p, tweets_corpus.content, tweets_corpus.polarity_bin, cv=5)\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71975497702909652"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t = tweets_test.polarity_bin.values\n",
    "result = np.abs(y_t - y)\n",
    "np.bincount(result)[0]/float(result.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
